{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimizations.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMi/+MOTeYoN7895oYozPKV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adeeconometrics/literate-programming/blob/main/Optimizations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCtpP6EwHZ_Q"
      },
      "source": [
        "# Symbolic and Numerical Computation\n",
        "By Dave Arthur D. Amiana \n",
        "\n",
        "----\n",
        "Overview of the notebook:\n",
        "\n",
        "This notebook will serve as an introduction to Optimization from single-variable to mutlivariable scenarios. For this notebook, optimization problems will be laid out from an engineering's perspective. Concepts will be briefly explained as well, but links to more elaborate discussions will be provided instead.\n",
        "\n",
        "The primary focus of this notebook is to gain some understanding with the concept and implement them in Python using the scientific libraries for numerical and symbolic computations. \n",
        "\n",
        "----\n",
        "\n",
        "### List of contents:\n",
        "- Single-variable functions\n",
        "    - Slope of a function\n",
        "        - Secant\n",
        "        - Tangent \n",
        "    - Newton's Method of approximation\n",
        "    - Concavity and Convexity of a function\n",
        "        - First Derivative test\n",
        "        - Second Derivative test \n",
        "\n",
        "- Multi-variable functions\n",
        "    - Partial derivatives\n",
        "    - Gradient of a function\n",
        "    - Gradient vector\n",
        "    - Jacobi Matrix\n",
        "    - Hessian Matrix\n",
        "    - Langrangian Multiplier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3aiRFwQHWWH"
      },
      "source": [
        "import numpy as np\n",
        "import scipy as sci\n",
        "import matplotlib.pyplot as plt\n",
        "import sympy as sp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3HWa19RIMML"
      },
      "source": [
        "from sympy import *\n",
        "sp.init_printing()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJgYlBo8IRnA"
      },
      "source": [
        "# initialize symbols\n",
        "x,y,z = sp.symbols(\"x y z\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEUien3nKCGZ"
      },
      "source": [
        "## Single Variable Functions\n",
        "Single-variable (univariable) functions are of the form $f(x)$ such that a function $f$ takes one *argument* $x$ and *maps it to* $y$. It is often written as: $f:x\\to y$. Albeit univariate functions are hardly used in the real world, taking some time to understand their behavior is rudimentary to understanding the behavior of multi-variable functions that we usually encounter in the wild.  \n",
        "\n",
        "----\n",
        "\n",
        "**Discussion:**\n",
        "\n",
        "Since optimization is concerned with finding the maximum and minimum values for a given function. These values, which we refer to as *extremas* of a function requires us to traverse over the function and point out the maximum point where the function maps to i.e. the highest peak of the shape of our function, and the lowest value our function can get. We can express our objective as ``max(f(x))`` and ``min(f(x))`` which translates to finding the maximum/minimum value of our function given the derinition of our function ``f(x)``, but that's not the whole story underneath. In order to find the maximum values of our function, we have to get a sense of what does it mean to be the highest and lowest value of a function? \n",
        "\n",
        "Our objective is to find the regions where it maps to the maximum and minimum values of $y$ given $x$ and compare this for all values of $x\\in D \\text{ where D is the domain of our function}$. In other words, we wish to find $\\forall x. [f(c)\\geq f(x)\\; | (x,c) \\in D]$ for our maximum value and  $\\forall x. [f(c)\\leq f(x)\\; | (x,c) \\in D]$ for our minimum value.\n",
        "\n",
        "\n",
        "Before we get into more details of this, it is good to begin with the notion of derivatives and how it ought to describe very little changes on our function. \n",
        "\n",
        "Recall the definition of the slope of a function $f(x)=y$, we denote the slope of $f(x)$ as $m=\\frac{\\Delta y}{\\Delta x}$ such that $\\Delta x$ desribes the changes in $x$ and $\\Delta y$ describes the changes that happens in $y$. Hence, the slope of a function, written in two-point form is denoted as $m = \\frac{y_2-y_1}{x_2 -x_1}$. But we wish to know the *very little changes* happening in a specific point $(x_0, y_0)$, our slope can only give us the change for a given interval. How can we generalize this notion to capture a richer sense of what's happening to our function when little values of $x_0$ changes? \n",
        "\n",
        "The derivative of a function of real variable i.e. $x\\in \\mathbb{R}$ measures the *sensitivity to change of the output value* with respect to a change in its *input value*, $x$.\n",
        "\n",
        "<!-- insert illustration -->\n",
        "\n",
        "The **derivative of a function** at a chosen input value, **when it exists**, is *the slope of the tangent line to the graph of the function at that point*. The **tangent line** is the best linear approximation of the function near that input value.\n",
        "\n",
        "For this reason, the derivative is often described as the instantaneous rate of change, the ratio of the instantaneous change in the dependent variable to that of the independent variable.\n",
        "\n",
        "----\n",
        "\n",
        "Examples:\n",
        "\n",
        "Notes for the following code blocks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRg9p2eRq2jf"
      },
      "source": [
        "#### Defnitions\n",
        "Extemas:\n",
        "- Absolute maximum\n",
        "- Local maximum\n",
        "- Absolute minimum\n",
        "- Local minimum\n",
        "\n",
        "Concavity\n",
        "\n",
        "Convexity\n",
        "\n",
        "Critical number - a critical number of a function $f$ is a number $c$ in the domain of $f$ such that either $f''(c)=0$ or $f'(c)$ does not exist. If $f$ has a local minimum or maximum at $c$, then $c$ is a critical number of $f$.\n",
        "\n",
        "----\n",
        "#### Theorems\n",
        "\n",
        "----\n",
        "#### Procedure\n",
        "- Increasing/Decreasing Test.\n",
        "    - if $f'(x)>0$ on an open interval, then $f$ is increasing on that interval. \n",
        "    - if $f'(x)<0$ on an open interval, then $f$ is decreasing on that interval.\n",
        "- First Derivative Test. Suppose that $c$ is a *critical number* of a continuous function $f$.\n",
        "    - if $f'$ changes from positive to negative at $c$, then $f$ has a local maximum at $c$.\n",
        "    - if $f'$ changes from negative to postive at $c$, then $f$ has a local minimum at $c$.\n",
        "    - if $f'$ does not change sign at $c$, that is, $f'$ is positive on both sides of $c$ or negative on both sides, then $f$ has no local maximum or minimum at $c$.\n",
        "\n",
        "\n",
        "- Second Derivative Test. Suuppose $f''$ is continuous near $c$.\n",
        "    - if $f'(c)=0$ and $f''(c)>0$, then $f$ has a local minimum at $c$.\n",
        "    - if $f'(c)=0$ and $f''(c)<0$, then $f$ has a local maximum at $c$\n",
        "\n",
        "- Concavity Test\n",
        "    - if $f''(x)>0$ for all $x$ in the interval $I$, then the graph of $f$ is **concave upward** on $I$.\n",
        "    - if $f''(x)>0$ for all $x$ in $I$, then the graph of $f$ is **concave downward** on $I$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_xbwfgUKTVw"
      },
      "source": [
        "# algorithm for finding the extreme values of a function\n",
        "''''\n",
        "applying the first derivative test, second derivative test, finding critical values, solving for critical values\n",
        "'''\n",
        "\n",
        "def in_dec_test(x):\n",
        "    pass\n",
        "\n",
        "def first_diff_test(x):\n",
        "    pass\n",
        "\n",
        "def second_diff_test(x):\n",
        "    pass \n",
        "\n",
        "def concavity_test(x):\n",
        "    pass\n",
        "\n",
        "def plot_funct(x,y):\n",
        "    x_val = np.linspace(-x,x,0.0001)\n",
        "    plt.plot(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL8nbauSKVOP"
      },
      "source": [
        "## Multivariable Functions\n",
        "\n",
        "Discussion: \n",
        "\n",
        "Examples:\n",
        "\n",
        "Notes for the following code blocks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vax7b9o1KaDo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}